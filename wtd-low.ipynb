{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Wind Turbine Detection","metadata":{"id":"Y6vQF8yFHP8-"}},{"cell_type":"markdown","source":"# Data Description\n\nThe dataset was taken from: https://www.kaggle.com/datasets/saurabhshahane/wind-turbine-obj-detection\n\nThe dataset contains a set of 1285 overhead images of wind turbines with corresponding YOLO formatted bounding boxes for object detection. These labels contain the class, x and y coordinates and the height and width of the bounding boxes for each wind turbine in the corresponding image.","metadata":{"id":"Qt2CsG8ZHVuP"}},{"cell_type":"markdown","source":"# Importing necessary libraries & loading data","metadata":{"id":"tk60QkV3HYhZ"}},{"cell_type":"code","source":"! pip install albumentations==0.4.6","metadata":{"id":"fh6au0p2HWU7","outputId":"302d8dc8-f3d9-46c9-9c4c-f571cde6958a","execution":{"iopub.status.busy":"2022-06-01T11:50:56.585133Z","iopub.execute_input":"2022-06-01T11:50:56.587684Z","iopub.status.idle":"2022-06-01T11:51:10.370482Z","shell.execute_reply.started":"2022-06-01T11:50:56.587581Z","shell.execute_reply":"2022-06-01T11:51:10.369381Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! pip install effdet","metadata":{"id":"fdt-vaWeH3XZ","outputId":"385023b9-b1ed-4419-cc38-7e1775d0891b","execution":{"iopub.status.busy":"2022-06-01T11:51:14.576777Z","iopub.execute_input":"2022-06-01T11:51:14.577170Z","iopub.status.idle":"2022-06-01T11:51:57.640865Z","shell.execute_reply.started":"2022-06-01T11:51:14.577132Z","shell.execute_reply":"2022-06-01T11:51:57.639736Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"! pip install ensemble-boxes","metadata":{"id":"oQa2uBQ8EMwl","outputId":"9907b4ea-4a72-4865-d3ba-12ac93fd42d6","execution":{"iopub.status.busy":"2022-06-01T11:51:57.643205Z","iopub.execute_input":"2022-06-01T11:51:57.643588Z","iopub.status.idle":"2022-06-01T11:52:07.409902Z","shell.execute_reply.started":"2022-06-01T11:51:57.643551Z","shell.execute_reply":"2022-06-01T11:52:07.408899Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm.autonotebook import tqdm, trange\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.image import imread\nimport copy\nimport time\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations.augmentations.functional as F\nfrom pathlib import Path\nimport PIL\nfrom effdet.config.model_config import efficientdet_model_param_dict\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\nfrom effdet.config.model_config import efficientdet_model_param_dict\nfrom ensemble_boxes import ensemble_boxes_wbf\nimport warnings\nfrom datetime import datetime\nfrom glob import glob","metadata":{"id":"85XgGq3nIKJJ","outputId":"9e630fc1-167f-456a-e22e-703335c3d875","execution":{"iopub.status.busy":"2022-06-01T11:52:28.899488Z","iopub.execute_input":"2022-06-01T11:52:28.899948Z","iopub.status.idle":"2022-06-01T11:52:39.753050Z","shell.execute_reply.started":"2022-06-01T11:52:28.899907Z","shell.execute_reply":"2022-06-01T11:52:39.751932Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"So the pipeline will contain the following steps: resize all images and bounding boxes to a 256x256 size for model training --> predict bounding boxes on unseen data with 256x256 size during inference --> resize predicted images with bounding boxes to the size of original images.","metadata":{"id":"oAdvXN5vIbm2"}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"id":"PbcVtUS6IfqH"}},{"cell_type":"code","source":"seed = 7788\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)\n\n# splitting the dataset into train, validation and test parts\nimages_dir = '../input/wind-turbine-obj-detection/images/images'\nboxes_dir = '../input/wind-turbine-obj-detection/labels/labels'\nimages_filenames = [filename[:-4]+'.jpg' for filename in list(os.listdir(boxes_dir)) if filename[:-4]+'.jpg' in list(os.listdir(images_dir))]\nprint('Total number of images in the dataset: {}'.format(len(images_filenames)))\n\n\nrandom.shuffle(images_filenames)\n\ntest_size = 10\ntrain_val_size = len(images_filenames) - test_size\nindexes = list(range(train_val_size))\nval_part = 0.2\nsplit = int(np.floor(train_val_size * val_part))\n\ntrain_images_filenames = images_filenames[split:train_val_size]\nval_images_filenames = images_filenames[:split]\ntest_images_filenames = images_filenames[-test_size:]\n\nprint('Splitted the dataset into 3 parts: {} train images, {} validation images, {} test images'.format(\n    len(train_images_filenames), len(val_images_filenames), len(test_images_filenames)))","metadata":{"id":"1tQCvn0TIdm4","outputId":"ebd20396-2d27-4165-aae7-6d5e8f13ebc1","execution":{"iopub.status.busy":"2022-06-01T11:53:02.190242Z","iopub.execute_input":"2022-06-01T11:53:02.191207Z","iopub.status.idle":"2022-06-01T11:53:03.382763Z","shell.execute_reply.started":"2022-06-01T11:53:02.191162Z","shell.execute_reply":"2022-06-01T11:53:03.381589Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# function for converting bounding boxes from yolo format to pascal voc format\ndef yolo_to_voc(yolo_box, image_width, image_height):\n    x_c, y_c, w, h = yolo_box\n    x_tl = x_c - w / 2\n    y_tl = y_c - h / 2\n    x_tl *= image_width\n    y_tl *= image_height\n    w *= image_width\n    h *= image_height\n    x_br = x_tl + w\n    y_br = y_tl + h\n    voc_box = np.array([x_tl, y_tl, x_br, y_br], dtype=np.int64)\n    voc_box = list(voc_box)\n    return voc_box","metadata":{"id":"1772s0PNIjaJ","execution":{"iopub.status.busy":"2022-06-01T11:56:33.867607Z","iopub.execute_input":"2022-06-01T11:56:33.868048Z","iopub.status.idle":"2022-06-01T11:56:33.874551Z","shell.execute_reply.started":"2022-06-01T11:56:33.868014Z","shell.execute_reply":"2022-06-01T11:56:33.873571Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# function for converting bounding boxes from yolo format to format required by Efficientdet architecture\ndef convert_to_effdet(box, image_width, image_height, format):\n    if format == 'yolo':\n        voc_box = yolo_to_voc(box, image_width, image_height)\n    elif format == 'pascal_voc':\n        voc_box = box\n    effdet_order = [1, 0, 3, 2]\n    effdet_box = [int(voc_box[i]) for i in effdet_order]\n    return effdet_box","metadata":{"id":"f0XeRplkIpOB","execution":{"iopub.status.busy":"2022-06-01T11:56:36.499558Z","iopub.execute_input":"2022-06-01T11:56:36.500286Z","iopub.status.idle":"2022-06-01T11:56:36.506210Z","shell.execute_reply.started":"2022-06-01T11:56:36.500249Z","shell.execute_reply":"2022-06-01T11:56:36.505189Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# function for visualization of bounding boxes\ndef visualize_box(image, box, color=(255, 0, 0), thickness=2):\n    x_min, x_max, y_min, y_max = int(box[1]), int(box[3]), int(box[0]), int(box[2])\n    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n    return image","metadata":{"id":"bo0o0P5BIrkD","execution":{"iopub.status.busy":"2022-06-01T11:56:39.754327Z","iopub.execute_input":"2022-06-01T11:56:39.754727Z","iopub.status.idle":"2022-06-01T11:56:39.760976Z","shell.execute_reply.started":"2022-06-01T11:56:39.754692Z","shell.execute_reply":"2022-06-01T11:56:39.759845Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# function for visualization of images with ground truth boxes and predictions\ndef show_images(images_filenames, images_dir, boxes_dir):\n    cols = 2\n    rows = len(images_filenames)\n    figure, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 50))\n    for i, image_filename in enumerate(images_filenames):\n        image = cv2.imread(os.path.join(images_dir, image_filename))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_width, image_height = image.shape[0], image.shape[1]\n        axes[i, 0].imshow(image)\n        axes[i, 0].set_title('Image')\n        axes[i, 0].set_axis_off()\n\n        boxes = np.loadtxt(os.path.join(boxes_dir, image_filename[:-4]+'.txt'), delimiter=' ')\n        if boxes.ndim < 2:\n            boxes = boxes[np.newaxis, :]\n        boxes = boxes[:, 1:]\n        converted_boxes = [convert_to_effdet(box, image_width, image_height, format='yolo') for box in boxes]\n        for box in converted_boxes:\n            visualize_box(image, box)\n        axes[i, 1].imshow(image)\n        axes[i, 1].set_title('Ground truth boxes')\n        axes[i, 1].set_axis_off()\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"mFCWXihDIuZj","execution":{"iopub.status.busy":"2022-06-01T11:56:42.558903Z","iopub.execute_input":"2022-06-01T11:56:42.559287Z","iopub.status.idle":"2022-06-01T11:56:42.569263Z","shell.execute_reply.started":"2022-06-01T11:56:42.559256Z","shell.execute_reply":"2022-06-01T11:56:42.568094Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"show_images(test_images_filenames, images_dir, boxes_dir)","metadata":{"id":"3nUPmXS0Iw8C","outputId":"50b9b07b-a75d-4b96-f298-ff43955426b0","execution":{"iopub.status.busy":"2022-06-01T11:57:17.116942Z","iopub.execute_input":"2022-06-01T11:57:17.117358Z","iopub.status.idle":"2022-06-01T11:57:20.329157Z","shell.execute_reply.started":"2022-06-01T11:57:17.117325Z","shell.execute_reply":"2022-06-01T11:57:20.327763Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Wind_Turbines_Dataset(Dataset):\n    def __init__(self, images_filenames, images_dir, boxes_dir, transform=None):\n        self.images_filenames = images_filenames\n        self.images_dir = images_dir\n        self.boxes_dir = boxes_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filenames)\n\n    def __getitem__(self, index):\n        image_filename = self.images_filenames[index]\n        image = cv2.imread(os.path.join(images_dir, image_filename))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_height, image_width = image.shape[0], image.shape[1]\n        boxes = np.loadtxt(os.path.join(boxes_dir, image_filename[:-4]+'.txt'), delimiter=' ')\n        if boxes.ndim < 2:\n            boxes = boxes[np.newaxis, :]\n        boxes = boxes[:, 1:]\n        # the dataset contains only 1 class - wind turbine\n        labels = torch.ones((boxes.shape[0], ), dtype=torch.int64)\n        boxes = [yolo_to_voc(box, image_width=image_width, image_height=image_height) for box in boxes]\n        if self.transform:\n            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n            image = transformed['image']\n            image_height, image_width = image.shape[1], image.shape[2]\n            boxes = transformed['bboxes']\n            labels = transformed['labels']\n        # converting bounding boxes to format required by Efficientdet architecture\n        boxes = [convert_to_effdet(box, image_width=image_width, image_height=image_height, format='pascal_voc') for box in boxes]\n        target = {'bboxes': torch.as_tensor(boxes),\n                  'labels': torch.as_tensor(labels, dtype=torch.int64),\n                  'image_id': torch.Tensor([index]),\n                  'img_size': (image_height, image_width),\n                  'img_scale': torch.Tensor([1.0])}\n        return image, target","metadata":{"id":"NaN18C49IzQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data augmentation\ntrain_transform = A.Compose(\n    [A.Resize(256, 256),\n     A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2),\n     A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20),\n     A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n     A.HorizontalFlip(),\n     A.VerticalFlip(),\n     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n     ToTensorV2()],\n     bbox_params=A.BboxParams(format='pascal_voc', min_area=0, min_visibility=0, label_fields=['labels']))\n\nval_transform = A.Compose(\n    [A.Resize(256, 256),\n     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n     ToTensorV2()],\n     bbox_params=A.BboxParams(format='pascal_voc', min_area=0, min_visibility=0, label_fields=['labels']))","metadata":{"id":"V3YOUrxJJiKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Wind_Turbines_Dataset(train_images_filenames, images_dir, boxes_dir, train_transform)\nval_dataset = Wind_Turbines_Dataset(val_images_filenames, images_dir, boxes_dir, val_transform)","metadata":{"id":"qvJZ_nUPJk6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom collate_fn function to allow usage of bounding boxes with different size in a batch\ndef collate_fn(batch):\n  images, targets = tuple(zip(*batch))\n  images = torch.stack(images)\n  images = images.float()\n  boxes = [target['bboxes'].float() for target in targets]\n  labels = [target['labels'].float() for target in targets]\n  img_size = torch.tensor([target['img_size'] for target in targets]).float()\n  img_scale = torch.tensor([target['img_scale'] for target in targets]).float()\n  annotations = {'bbox': boxes,\n                 'cls': labels,\n                 'img_size': img_size,\n                 'img_scale': img_scale}\n  return images, annotations","metadata":{"id":"IrqbRh2hJnVy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)","metadata":{"id":"oONEZoyzme7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, annotations = next(iter(train_loader))\nlen(images)","metadata":{"id":"H7RHeJf8mgMr","outputId":"28048abc-7843-4683-8e1f-9bb3269eb328"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for image denormalization in order to visualize it correctly\ndef denormalize(image):\n  inverse_norm = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n                                      std=[1/0.229, 1/0.224, 1/0.255])\n  denormalized_image = inverse_norm(image)\n  return denormalized_image","metadata":{"id":"8OhLrJJVmmyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for visualization of augmented images and their bounding boxes\ndef augmentation_viz(dataset, index=0, samples=5):\n  dataset = copy.deepcopy(dataset)\n  fig, axes = plt.subplots(nrows=samples, ncols=1, figsize=(10, 30))\n  for i in range(samples):\n    image, target = dataset[index]\n    boxes = target['bboxes']\n    image = denormalize(image)\n    image = image.permute(1, 2, 0).numpy()\n    for box in boxes:\n      visualize_box(image, box, thickness=1)\n    axes[i].imshow(image)\n    axes[i].set_title('Augmented image')\n    axes[i].set_axis_off()\n  plt.tight_layout()\n  plt.show()","metadata":{"id":"qr02iE1CmnkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmentation_viz(train_dataset, index=25)","metadata":{"id":"53kqFKIXmqLK","outputId":"d5ecd456-9d24-49c3-a606-b4415639e99e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{"id":"tePkJxDfmwvx"}},{"cell_type":"code","source":"# Computes and stores the average and current value\n# class AverageMeter(object):\n#     def __init__(self):\n#         self.reset()\n\n#     def reset(self):\n#         self.val = 0\n#         self.avg = 0\n#         self.sum = 0\n#         self.count = 0\n\n#     def update(self, val, n=1):\n#         self.val = val\n#         self.sum += val * n\n#         self.count += n\n#         self.avg = self.sum / self.count","metadata":{"id":"29dOMEFsmKwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Computes and stores the average and current value\nclass AverageMeter(object):\n  def __init__(self):\n    self.reset()\n\n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n\n  def update(self, val, n=1):\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count","metadata":{"id":"rD6EpxiL_SSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\")\n\n# class Fitter:\n#     def __init__(self, model, device, config):\n#         self.config = config\n#         self.epoch = 0\n\n#         self.base_dir = f'./{config.folder}'\n#         if not os.path.exists(self.base_dir):\n#             os.makedirs(self.base_dir)\n        \n#         self.log_path = f'{self.base_dir}/log.txt'\n#         self.best_summary_loss = 10**5\n\n#         self.model = model\n#         self.device = device\n\n#         param_optimizer = list(self.model.named_parameters())\n#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n#         optimizer_grouped_parameters = [\n#             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n#             {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n#         ] \n\n#         self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n#         self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n#         self.log(f'Fitter prepared. Device is {self.device}')\n\n#     def fit(self, train_loader, validation_loader):\n#         for e in range(self.config.n_epochs):\n#             if self.config.verbose:\n#                 lr = self.optimizer.param_groups[0]['lr']\n#                 timestamp = datetime.utcnow().isoformat()\n#                 self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n#             t = time.time()\n#             summary_loss = self.train_one_epoch(train_loader)\n\n#             self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n#             self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n#             t = time.time()\n#             summary_loss = self.validation(validation_loader)\n\n#             self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n#             if summary_loss.avg < self.best_summary_loss:\n#                 self.best_summary_loss = summary_loss.avg\n#                 self.model.eval()\n#                 self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n#                 for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n#                     os.remove(path)\n\n#             if self.config.validation_scheduler:\n#                 self.scheduler.step(metrics=summary_loss.avg)\n\n#             self.epoch += 1\n\n#     def validation(self, val_loader):\n#         self.model.eval()\n#         summary_loss = AverageMeter()\n#         t = time.time()\n#         for step, (images, annotations) in enumerate(val_loader):\n#             if self.config.verbose:\n#                 if step % self.config.verbose_step == 0:\n#                     print(\n#                         f'Val Step {step}/{len(val_loader)}, ' + \\\n#                         f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n#                         f'time: {(time.time() - t):.5f}', end='\\r'\n#                     )\n#             with torch.no_grad():\n#                 images = images.to(self.device)\n#                 batch_size = images.shape[0]\n#                 annotations['bbox'] = [box.to(self.device).float() for box in annotations['bbox']]\n#                 annotations['cls'] = [cls.to(self.device).float() for cls in annotations['cls']]\n#                 annotations['img_size'] = annotations['img_size'].to(self.device)\n#                 annotations['img_scale'] = annotations['img_scale'].to(self.device)\n\n#                 loss  = self.model(images, annotations)\n#                 loss = loss['loss']\n#                 summary_loss.update(loss.detach().item(), batch_size)\n\n#         return summary_loss\n\n#     def train_one_epoch(self, train_loader):\n#         self.model.train()\n#         summary_loss = AverageMeter()\n#         t = time.time()\n#         for step, (images, annotations) in enumerate(train_loader):\n#             if self.config.verbose:\n#                 if step % self.config.verbose_step == 0:\n#                     print(\n#                         f'Train Step {step}/{len(train_loader)}, ' + \\\n#                         f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n#                         f'time: {(time.time() - t):.5f}', end='\\r'\n#                     )\n            \n#             images = images.to(self.device)\n#             batch_size = images.shape[0]\n#             annotations['bbox'] = [box.to(self.device).float() for box in annotations['bbox']]\n#             annotations['cls'] = [cls.to(self.device).float() for cls in annotations['cls']]\n\n#             self.optimizer.zero_grad()\n            \n#             loss  = self.model(images, annotations)\n#             loss = loss['loss']\n#             loss.backward()\n\n#             summary_loss.update(loss.detach().item(), batch_size)\n\n#             self.optimizer.step()\n\n#             if self.config.step_scheduler:\n#                 self.scheduler.step()\n\n#         return summary_loss\n    \n#     def save(self, path):\n#         self.model.eval()\n#         torch.save({\n#             'model_state_dict': self.model.model.state_dict(),\n#             'optimizer_state_dict': self.optimizer.state_dict(),\n#             'scheduler_state_dict': self.scheduler.state_dict(),\n#             'best_summary_loss': self.best_summary_loss,\n#             'epoch': self.epoch,\n#         }, path)\n\n#     def load(self, path):\n#         checkpoint = torch.load(path)\n#         self.model.model.load_state_dict(checkpoint['model_state_dict'])\n#         self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#         self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n#         self.best_summary_loss = checkpoint['best_summary_loss']\n#         self.epoch = checkpoint['epoch'] + 1\n        \n#     def log(self, message):\n#         if self.config.verbose:\n#             print(message)\n#         with open(self.log_path, 'a+') as logger:\n#             logger.write(f'{message}\\n')","metadata":{"id":"8UzwSsSGmXLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\nclass Fitter:\n  def __init__(self, model, device, config):\n    self.config = config\n    self.epoch = 0\n    self.base_dir = f'./{config.folder}'\n    if not os.path.exists(self.base_dir):\n      os.makedirs(self.base_dir)\n    self.log_path = f'{self.base_dir}/log.txt'\n    self.best_summary_loss = 10**5\n    self.model = model\n    self.device = device\n    param_optimizer = list(self.model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n                                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}] \n\n    self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n    self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n    self.log(f'Fitter prepared. Device is {self.device}')\n\n  def fit(self, train_loader, validation_loader):\n    for e in range(self.config.n_epochs):\n      if self.config.verbose:\n        lr = self.optimizer.param_groups[0]['lr']\n        timestamp = datetime.utcnow().isoformat()\n        self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n      t = time.time()\n      summary_loss = self.train_one_epoch(train_loader)\n      self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n      self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n      t = time.time()\n      summary_loss = self.validation(validation_loader)\n\n      self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n      if summary_loss.avg < self.best_summary_loss:\n        self.best_summary_loss = summary_loss.avg\n        self.model.eval()\n        self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n        for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n          os.remove(path)\n\n      if self.config.validation_scheduler:\n        self.scheduler.step(metrics=summary_loss.avg)\n\n      self.epoch += 1\n\n  def validation(self, val_loader):\n    self.model.eval()\n    summary_loss = AverageMeter()\n    t = time.time()\n    for step, (images, annotations) in enumerate(val_loader):\n      if self.config.verbose:\n        if step % self.config.verbose_step == 0:\n          print(f'Val Step {step}/{len(val_loader)}, ' + \\\n                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                f'time: {(time.time() - t):.5f}', end='\\r')\n      with torch.no_grad():\n        images = images.to(self.device)\n        batch_size = images.shape[0]\n        annotations['bbox'] = [box.to(self.device).float() for box in annotations['bbox']]\n        annotations['cls'] = [cls.to(self.device).float() for cls in annotations['cls']]\n        annotations['img_size'] = annotations['img_size'].to(self.device)\n        annotations['img_scale'] = annotations['img_scale'].to(self.device)\n\n        loss  = self.model(images, annotations)\n        loss = loss['loss']\n        summary_loss.update(loss.detach().item(), batch_size)\n\n    return summary_loss\n\n  def train_one_epoch(self, train_loader):\n    self.model.train()\n    summary_loss = AverageMeter()\n    t = time.time()\n    for step, (images, annotations) in enumerate(train_loader):\n      if self.config.verbose:\n        if step % self.config.verbose_step == 0:\n          print(f'Train Step {step}/{len(train_loader)}, ' + \\\n                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                f'time: {(time.time() - t):.5f}', end='\\r')\n            \n      images = images.to(self.device)\n      batch_size = images.shape[0]\n      annotations['bbox'] = [box.to(self.device).float() for box in annotations['bbox']]\n      annotations['cls'] = [cls.to(self.device).float() for cls in annotations['cls']]\n\n      self.optimizer.zero_grad()\n            \n      loss  = self.model(images, annotations)\n      loss = loss['loss']\n      loss.backward()\n\n      summary_loss.update(loss.detach().item(), batch_size)\n\n      self.optimizer.step()\n\n      if self.config.step_scheduler:\n        self.scheduler.step()\n\n    return summary_loss\n    \n  def save(self, path):\n    self.model.eval()\n    torch.save({'model_state_dict': self.model.model.state_dict(),\n                'optimizer_state_dict': self.optimizer.state_dict(),\n                'scheduler_state_dict': self.scheduler.state_dict(),\n                'best_summary_loss': self.best_summary_loss,\n                'epoch': self.epoch}, path)\n\n  def load(self, path):\n    checkpoint = torch.load(path)\n    elf.model.model.load_state_dict(checkpoint['model_state_dict'])\n    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    self.best_summary_loss = checkpoint['best_summary_loss']\n    self.epoch = checkpoint['epoch'] + 1\n        \n  def log(self, message):\n    if self.config.verbose:\n      print(message)\n    with open(self.log_path, 'a+') as logger:\n      logger.write(f'{message}\\n')","metadata":{"id":"rASz003o_qbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class TrainGlobalConfig:\n#     num_workers = 1\n#     batch_size = 16 \n#     n_epochs = 40\n#     lr = 0.007\n\n#     folder = 'effdet_d2'\n\n#     verbose = True\n#     verbose_step = 1\n#     step_scheduler = False  # do scheduler.step after optimizer.step\n#     validation_scheduler = True  # do scheduler.step after validation stage loss\n    \n#     SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n#     scheduler_params = dict(\n#         mode='min',\n#         factor=0.5,\n#         patience=1,\n#         verbose=False, \n#         threshold=0.0001,\n#         threshold_mode='abs',\n#         cooldown=0, \n#         min_lr=1e-8,\n#         eps=1e-08\n#     )","metadata":{"id":"DuMHU5YznH7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainGlobalConfig:\n  num_workers = 1\n  batch_size = 16 \n  n_epochs = 40\n  lr = 0.007\n\n  folder = 'effdet_d2'\n\n  verbose = True\n  verbose_step = 1\n  step_scheduler = False  # do scheduler.step after optimizer.step\n  validation_scheduler = True  # do scheduler.step after validation stage loss\n    \n  SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n  scheduler_params = dict(mode='min',\n                          factor=0.5,\n                          patience=1,\n                          verbose=False,\n                          threshold=0.0001,\n                          threshold_mode='abs',\n                          cooldown=0,\n                          min_lr=1e-8,\n                          eps=1e-08)","metadata":{"id":"2tbgANuGDdYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_training():\n  device = torch.device('cuda:0')\n  net.to(device)\n  fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n  fitter.fit(train_loader, val_loader)","metadata":{"id":"yZroSEOinXoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(num_classes=1, image_size=256, name='efficientdet_d2', backbone_name='efficientnet_b2'):\n  efficientdet_model_param_dict[name] = dict(name=name,\n                                             backbone_name=backbone_name,\n                                             backbone_args=dict(drop_path_rate=0.2),\n                                             num_classes=num_classes,\n                                             url='')\n  config = get_efficientdet_config(name)\n  config.update({'num_classes': num_classes})\n  config.update({'image_size': (image_size, image_size)})\n  net = EfficientDet(config, pretrained_backbone=True)\n  net.class_net = HeadNet(config, num_outputs=num_classes)\n  return DetBenchTrain(net, config)\n\nnet = build_model()","metadata":{"id":"OMCm7ERkniEB","outputId":"42c71e5f-e083-4b29-8dc8-4cf72d39389c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_training()","metadata":{"id":"AzYjx1wXntLh","outputId":"9a2df7ee-480b-42a8-8841-8e80cca7d121"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(net.state_dict(), 'trained_effdet_low')","metadata":{"id":"1tMqDS7crDJx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"hL8-CLrLDP4l"}},{"cell_type":"code","source":"class Turbine_Inference_Dataset(Dataset):\n  def __init__(self, images_filenames, images_dir, transform=None):\n    self.images_filenames = images_filenames\n    self.images_dir = images_dir\n    self.transform = transform\n\n  def __len__(self):\n    return len(self.images_filenames)\n\n  def __getitem__(self, index):\n    image_filename = self.images_filenames[index]\n    image = cv2.imread(os.path.join(self.images_dir, image_filename))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    if self.transform:\n      transformed = self.transform(image=image)\n      image = transformed['image'] \n    return image","metadata":{"id":"Wz-KGfL_DRB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing transformations for test images\ntest_transform = A.Compose([A.Resize(256, 256),\n                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                            ToTensorV2()])\ntest_dataset = Turbine_Inference_Dataset(test_images_filenames, images_dir, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)","metadata":{"id":"i7FdukJ0DbXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking if test loader works correctly\nimages = next(iter(test_loader))\nlen(images)","metadata":{"id":"hzNnG46xDiSE","outputId":"cf68dab4-783e-4be4-ce11-3c4c3f5ec58e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images.shape","metadata":{"id":"RpfhoYewp-E0","outputId":"80a440ed-ca3a-47c9-907c-9b3640925ca0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"9bX2KPy4KjCX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model, images, score_threshold=0.2):\n  model.eval()\n  images = images.to(device)\n  dummy_targets = {'bbox': [torch.tensor([[0.0, 0.0, 0.0, 0.0]], device=device) for i in range(images.shape[0])],\n                     'cls': [torch.tensor([1.0], device=device) for i in range(images.shape[0])],\n                     'img_size': torch.tensor([(images.shape[2], images.shape[3])] * images.shape[0], device=device).float(),\n                     'img_scale': torch.ones(images.shape[0], device=device).float()}\n  predictions = []\n  with torch.no_grad():\n    det = model(images, dummy_targets)['detections']\n    for i in range(images.shape[0]):\n      boxes = det[i].detach().cpu().numpy()[:, :4]    \n      scores = det[i].detach().cpu().numpy()[:, 4]\n      classes = det[i].detach().cpu().numpy()[:, 5]\n      indexes = np.where(scores > score_threshold)[0]\n      boxes = boxes[indexes]\n      boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n      boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n      predictions.append({'boxes': boxes[indexes],\n                          'scores': scores[indexes],\n                          'classes': classes[indexes]})\n    return predictions","metadata":{"id":"gk_6XKwIDk1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_wbf(predictions, image_size=256, iou_thr=0.2, skip_box_thr=0.2, weights=None):\n    bboxes = []\n    confidences = []\n    class_labels = []\n\n    for prediction in predictions:\n        boxes = [(prediction['boxes'] / (image_size-1)).tolist()]\n        scores = [prediction['scores'].tolist()]\n        labels = [prediction['classes'].tolist()]\n\n        boxes, scores, labels = ensemble_boxes_wbf.weighted_boxes_fusion(\n            boxes,\n            scores,\n            labels,\n            weights=weights,\n            iou_thr=iou_thr,\n            skip_box_thr=skip_box_thr,\n        )\n        boxes = boxes * (image_size - 1)\n        bboxes.append(boxes.tolist())\n        confidences.append(scores.tolist())\n        class_labels.append(labels.tolist())\n\n    return bboxes, confidences, class_labels","metadata":{"id":"Nm6ZKH3cFf3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for images in tqdm(test_loader, leave = True):\n  predictions = make_predictions(net, images)\n  boxes, scores, labels = run_wbf(predictions)","metadata":{"id":"NkI60HQbHU2N","outputId":"52b35378-810c-4c2f-839c-9f7fb0b5c244"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_predictions(images_filenames, images_dir, boxes):\n  cols = 2\n  rows = len(images_filenames)\n  figure, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 50))\n  for i, image_filename in enumerate(images_filenames):\n    image = cv2.imread(os.path.join(images_dir, image_filename))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    original_height, original_width = image.shape[0], image.shape[1]\n    axes[i, 0].imshow(image)\n    axes[i, 0].set_title('Original image')\n    axes[i, 0].set_axis_off()\n\n    resized_image = test_dataset[i]\n    resized_image = denormalize(resized_image)\n    resized_image = resized_image.permute(1, 2, 0).numpy()\n    image_boxes = boxes[i]\n    for box in image_boxes:\n      cv2.rectangle(resized_image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color=(255, 0, 0), thickness=1)\n    full_sized_image = F.resize(resized_image, height=original_height, width=original_width, interpolation=cv2.INTER_NEAREST)\n    axes[i, 1].imshow(full_sized_image)\n    axes[i, 1].set_title('Detected objects')\n    axes[i, 1].set_axis_off()\n  plt.tight_layout()\n  plt.show()","metadata":{"id":"z0T57zbVFxRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_predictions(test_images_filenames, images_dir, boxes)","metadata":{"id":"15GLUQ9wtko0","outputId":"61e3fb46-ae17-4e4a-fe27-dee3b8238e61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8_0xstNOt_E2"},"execution_count":null,"outputs":[]}]}